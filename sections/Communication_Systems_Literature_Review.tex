\section{Communication Systems}
As modern communication systems move towards the deployment of 6G networks, the need for efficient and reliable communication systems has become increasingly important. The use of machine learning techniques in communication systems has been shown to improve the performance of wireless communication systems by optimizing the use of resources and reducing the latency of data transmission.

To address challenges in 6G communication systems novel techniques such as reconfigurable intilligence surfaces (RISs), integrated sensing and communication, mmWave and THz networks, and other methods have been proposed. \cite{zhou2023survey} Although these methods demonstrate satisfactory performance towards 6G requirements, the requirements for network management have increased in complexity and scale. The future 6G networks have leveraged machine learning (ML) as promising solution to network management techniques which optimize the use of resources and reduce the latency of data transmission. \cite{zhou2024large} To address the challenges in communication systems complexity there have been several studies that leverage techniques such as reinforcement learning \cite{zhou2021ran}, map assisted localization ray tracing techniques \cite{vuckovic2021map}, deep network assisted channel state information (CSI) prediction \cite{vuckovic2024csi}. These studies have demonstrated the potential of machine learning techniques in improving the performance of wireless communication systems. 

Recently, large language models (LLM) techniques have demonstrated the potential to improve through the use of massivily large models that demonstrate the ability to reason and comprehend complex tasks across various domains. The emergence of these models have shown to be effective in addressing a number of different challenges in the domains of health care, law, finance, education, and other technical fields.%\cite{Med-PaLM2, Bloomberg-GPT, etc}

From early in the development of language models, natural language research such as, GPT-2 showed that LLM technologies have emergent capabilities and are unsupervised multitask learners. \cite{radford2019language}. Since this seminal paper by Radform, et. al, the field of large language models has matured significantly, and new research has emerge on how to apply the capabilities of these models across various domains and different ways. The technique of \texit(prompt engineering) is a technique that gained prominence and served as a way to improve the performance of a model by crafting input prompts. By altering the phrasing or structure of prompts, the LLM can be fine-tuned to align the behavior with a desired task. \cite{liu2023pre} The concept of In-Context learning was explored in \cite{brown2020language} and demonstrated how LLMs can adapt to new tasks by utilizing contextual clues in the input prompt. To build on the capabilities of utilizing LLMs without the need for fine-tuning, the concept of \textit(Chain of Thought reasoning) CoT was introduced in \cite{huang2023language} and demonstrates how LLMs can guided through a sequence of intermeidate reasoning steps to solve complex problems. The CoT reasoning technique demonstrated substantial improvements in solving mathematical and logical reasoning tasks by breaking down the problem into smaller steps. 