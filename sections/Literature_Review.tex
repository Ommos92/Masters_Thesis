\section{6G Communication Systems}
As modern communication systems move towards the deployment of 6G networks, the need for efficient and reliable communication systems has become increasingly important. The use of machine learning techniques in communication systems has been shown to improve the performance of wireless communication systems by optimizing the use of resources and reducing the latency of data transmission.

To address challenges in 6G communication systems novel techniques such as reconfigurable intilligence surfaces (RISs), integrated sensing and communication, mmWave and THz networks, and other methods have been proposed. \cite{zhou2023survey} Although these methods demonstrate satisfactory performance towards 6G requirements, the requirements for network management have increased in complexity and scale. The future 6G networks have leveraged machine learning (ML) as promising solution to network management techniques which optimize the use of resources and reduce the latency of data transmission. \cite{zhou2024large} To address the challenges in communication systems complexity there have been several studies that leverage techniques such as reinforcement learning \cite{zhou2021ran}, map assisted localization ray tracing techniques \cite{vuckovic2021map}, deep network assisted channel state information (CSI) prediction \cite{vuckovic2024csi}. These studies have demonstrated the potential of machine learning techniques in improving the performance of wireless communication systems. While these prior techniques demonstrate potential in improving the performance of wireless communication systems, they often rely on physics based models or simulated environments, which can fall short in capturing the complexity and variability of real-world communication systems. These limitations hinder the applicability and robustness of developed solutions, particularly in dynamic environments. To overcome these challenges, the Deepsense 6G dataset was developed to provide researches with a large-scale, real-world dataset for millimeter-wave based communication systems.  The dataset provides various sensing scenarios, such as vehicle to infrastructure, drone communication, indoor and outdoor positioning, as well as a multitude of different task sets for 6G communication applications.\cite{alkhateeb2023deepsense} The dataset provides a diverse set of scenarios and environments, which enables researchers to explore an develop deep learning networks that directly apply to real-world scenarios. In the next section, we will review the literature on the Deepsense 6G dataset and the use of deep learning techniques for millimeter-wave based communication systems.

\section{DeepSense 6G Methods}
The Deepsense 6G dataset is a real-world multi-modal dataset that provides researchers a comprehensive dataset for multi-modal sensing and communication data. The dataset provides the world's first large-scale real-world sensing and communication reprository of over a million data points, with over 30 different scenarios that target multiple applications. The team that provided the DeepSense 6G dataset aims to provide a platform that encourages the development of machine learning solutions for applications in communication systems, through a variety of multi-modal sensor technologies. There are various tasks that are supported by the dataset as well, ranging from \textit{Sensing Aided Beam Prediction} to \textit{Future Blockage Prediction}, with each task supported by a diverse set of sensing modalities. The dataset provides multiple input modalities such as GPS, Camera, Radar, and LiDAR sensing modalities aimed to enhance the breadth of the dataset for various applications. In addition to the various data points, test benches, and scenarios, the Deepsense team provides a wide array of papers, code bases, and techniques for solving problems relevant to the dataset. The inception of the DeepSense 6G dataset has marked a significant milestone in fostering research within the field of communications, particularly in scenarios requiring multi-modal sensor integration. Before the introduction of datasets like DeepSense 6G, researchers encountered substantial challenges, predominantly due to the lack of access to large-scale, realistic datasets that reflect the complexity and variety of real-world environments. \cite{alkhateeb2023deepsense}

Traditional methods for beam management often rely on exhaustive beam search techniques, which often have drawbacks in high overhead and an intractible search space. Leveraging a large-scale dataset Machine Learning frameworks are viable alternative to traditional datascience techniques as outlined in the paper, \textit{Computer Vision Aided Beam Tracking in a Real-World Millimeter Wave Deployment}. In this work the authors propose to utilize temporal visual sensing information to predict the optimal beam, as defined as the highest beamforming gain through the use of an Encoder-Decoder network. The paper presents an innovative machine learning (ML) model for beam tracking optimization in mmWave communication systems, in which a base station is equipped with an antenna array and an RGB camera to assist mobile users. The proposed model leverages visual sensing information alongside pre-defined beamforming codebooks to predict optimal future beams using an encoder-decoder architecture leveraging Recurrent Neural Networks (RNNs). The vision-aided approach demonstrates promise over the traditional methods by leveraging existing feature extraction networks to enhance beamforming gain and communication performance. \cite{jiang2022computer} The paper underscores the effectiveness of machine learning-based and vision-aided beam tracking in mmWave communications. This approach shows that it is possible to achieve a high level of accuracy for narrowing the beam search to top-5 accuracy with a precision of 99.37 percent for the next beam by observing previous time-steps on a single scenario.

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.5\textwidth]{Images/Literature_Review/jiang2022computer_paper.png}
  \caption{The considered system model leveraged to design a computer vision aided Beam Tracking system \cite{jiang2022computer}.}
  \label{fig:my_label}
\end{figure}

The paper's focus on next beam prediction presents limited scope, and does not take into account the some of the deeper challenges often faced in wireless communication systems by limiting it's evaluation to a single scenario. In turn the paper fails to consider the variability of real-world environments. The omission of additional sensing modalities such as radar, GPS, or other sensing inputs might limit the system's capabilites in Non-Line of Sight conditions (NLOS), where additional information might be crucial. Other methods delve into the usage of position-aided beam prediction aiming to evaluate the practicality of GPS guided predictions. In the paper \textit{Position-aided Beam Prediction in the Real World: How Useful GPS Locations Actually Are?}, the authors propose three machine learning solutions for position-aided mmWave beam prediction. They evaluate multiple methods using a Lookup Table, K-Nearest Neighbors, and a fully connected Neural network against three different scenarios. The paper reveals that the Neural Network generally outperforms both the lookup table and K-Nearest Neighbors in top-1 predictions across various scenarios due to the ability of the Neural Network to generalize better and utilize more information from training samples. They do note that factors such as input noise and label noise degrades performance and necessitates alternate metrics such as power loss, but the paper outlines the practical implications of using position data for beam alignment. Additionally, the authors point out that utilizing GPS positions can provide significant savings for the overhead of beam search in a code book that contains 64 unique codes. \cite{morais2023position}. There are several drawbacks to the use of GPS for beam alignment and some of these challenges include susceptibility to noisy inputs, latency issues, and environmental constraints, which lead the authors to suggest techniques that do not solely rely on GPS based methods. 

Location-aware methods offer a practical approach but come with limitations such as latency issues, environmental blockages, and noisy inputs. Works by the DeepSense team explore the efficacy of using Radar Aided Beam Prediction in the paper \textit{Radar Aided 6G Beam Prediction: Deep Learning Algorithms and Real-World Demonstration}. The team proposes a novel radar-aided deep learning framework to map radar samples to the optimal beam predictions. The framework leverages preprocessing of the radar samples into feature maps such as range, angle, and velocity maps using Fast Fourier Transforms (FFTs) and then employ a deep neural network subsequently to learn the mapping from the features to optimal beamforming predictions. The network is designed to be a relatively simple deep network comprosing of convolutional and fully-connected layers utilizing the rectified linear unit (ReLU) non-linear activation functions. The final layer of the network maps a hidden-layer to the designed beam code book size of 64 positions in which the objective function utilizes cross-entropy loss for predicting the optimal beam. The promise for utilizing relatively simple deep learning networks and demonstrates the capabilities of the approach showing how a the model can acheive around 90 percent for top-5 accuracy. This research underscores the potential for utilizing radar for inferring beam prediction with the radar modality. \cite{demirhan2022radar} 

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.5\textwidth]{Images/Literature_Review/demirhan2022radar_architecture.png}
  \caption{The considered architecture leveraged implemented in the radar aided Beam tracking paper. \cite{jiang2022computer}.}
  \label{fig:my_label}
\end{figure}

In addition to radar, vision, or position based sensing methods, other modalities like LiDAR (Light Detection and Ranging) can contibute to the diversification of environmental perception. \textit{LiDAR Aided Future Beam Prediction in Real-World Millimeter Wave V2I Communications} explores the effectiveness of utilizing LiDAR in the Beam prediction and reduction tasks. The authors argue that vision-aided methods might fall short in low light conditions, and may cause privacy concerns when it comes to positional information of the end user. The paper investigates utilizing LiDAR for mmWave beam prediction and tracking tasks, which in turn shows promising results in predicting optimal beam at 95 percent for top 5 accuracy.  

% Look at one of the multimodal papers and discuss the results and the architecture

Although these methods are effective in solving the tasks given by the dataset, they are often bespoke to the specific modalities utilized, and limited in the capabilities to adapt to different scenarios, conditions, additional tasks, and may lack the necessary requirements for real-world 6G network infrastructure. Custom designed architectures, feature engineering, task specific networks, and other drawbacks in prior deep learning methods can hinder the scalability and prohibit the widespread adoption of deep learning tehcniques to commerical 6G communication infrastructure. Ultimately, the diverse range of needed capabilities for 6G communication systems require a more general and adaptable solution that can be applied to a wide range of tasks and scenarios.

A suitable technology has emerged and potentially address these challenges. The explosion in research and commerical application of large language models (LLMs) has demonstrated the potential to solve problems in a diverse set of fields ranging from medical to financial, and have attracted significant attention in the field of communication systems. The next section will discuss the fundamentals of Large Language Models, their internal architecture, inference and utilization, and how they are utilized. 

\section{Large Language Models}

Recently, large language models (LLM) techniques have demonstrated the potential to improve through the use of massivily large models that demonstrate the ability to reason and comprehend complex tasks across various domains. The emergence of these models have shown to be effective in addressing a number of different challenges in the domains of health care, law, finance, education, and other technical fields.%\cite{Med-PaLM2, Bloomberg-GPT, etc}
 
\subsection{Large Language Model Fundamentals}
A Brief overview of the fundamentals of Large Language Models (LLMs) and their capabilities.

\subsection{Model Architecture}
The fundamental architecture of large language models is based on the transformer scheme, which utilizes an attention mechanism to capture global dependencies between inputs and outputs. 

\subsubsection{Encoder Only Architecture}

\subsubsection{Encode-Decoder Architecture}

\subsubsection{Decoder Only Architecture}

\subsection{Large Language Model Fine-Tuning}

\subsection{Large Language Models Inference and Utilization}

From early in the development of language models, natural language research such as, GPT-2 showed that LLM technologies have emergent capabilities and are unsupervised multitask learners. \cite{radford2019language}. Since this seminal paper by Radform, et. al, the field of large language models has matured significantly, and new research has emerge on how to apply the capabilities of these models across various domains and different ways. The technique of \textit(prompt engineering) is a technique that gained prominence and served as a way to improve the performance of a model by crafting input prompts. By altering the phrasing or structure of prompts, the LLM can be fine-tuned to align the behavior with a desired task. \cite{liu2023pre} The concept of In-Context learning was explored in \cite{brown2020language} and demonstrated how LLMs can adapt to new tasks by utilizing contextual clues in the input prompt. To build on the capabilities of utilizing LLMs without the need for fine-tuning, the concept of \textit(Chain of Thought reasoning) CoT was introduced in \cite{huang2023language} and demonstrates how LLMs can guided through a sequence of intermeidate reasoning steps to solve complex problems. The CoT reasoning technique demonstrated substantial improvements in solving mathematical and logical reasoning tasks by breaking down the problem into smaller steps.


\section{Multimodal Large Language Models}
% Review some literature on multimodal LLMs and Vision Language Models


\section(Large Language Model Frameworks)
% Review some literature on the frameworks that are used to train and deploy LLMs such as Hugging Face, OpenAI, Langchain, and Autogen



\section{Foundational Model's future impact on 6G Communication Systems}
% Transition into how LLMs can be used in 6G Communication Systems
