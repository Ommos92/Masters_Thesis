\section{Large Language Models Literature Review}
% Start to write about the transitions from Deep Networks
% to Transformers and the evolution of the field
Previous studies have made significant contributions to addressing the Deepesense challenge problem sets, these methods primarily focused using deep learning techniques for millimeter-wave based communication systems. Researchers have proposed various machine learning-based approaches to address the challenges of wireless communication systems, however a these techniques are often limited by their reliance on dataset specific features. The reliance on pre-trained object detection models, such as YOLOv3, or Resnet50, may limit the network's ability to generalize to new, unseen data or "zero-shot" tasks in which the model has not been trained on similar data. In contrast, large language models have been shown to demonstrate emergent properties that allow them to generalize to new tasks and data. As demonstrated in \cite{radford2019language} language models such as GPT-2 have been shown to perform well on a wide range of tasks, including text generation, summarization, and question answering. These models have been shown to be capable of zero-shot learning, in which the model is able to perform well on tasks it has not been trained on. 



